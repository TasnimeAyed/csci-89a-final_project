{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRrmgI3cpw2D"
   },
   "source": [
    "\n",
    "# HU Extension    ---      Final Project    ---   S89A DL for NLP\n",
    "# Michael Lee & Micah Nickerson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvkytUBwm_mz"
   },
   "source": [
    "# PART 2B - ADVERSARIAL ATTACK GENERATOR\n",
    "\n",
    "This is a notebook used to create the different adversarial attack **word perturbations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1564872440209,
     "user": {
      "displayName": "Michael Lee",
      "photoUrl": "",
      "userId": "10845871506120063440"
     },
     "user_tz": 240
    },
    "id": "MifqNtbK9_6X",
    "outputId": "2a0ed307-d89c-42d2-eedf-5ffb0a84d36e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/Shared drives/CSCI S-89A - Group Project/Data Sets/adversarial_asap/valid_set_plus_ADVERSARIAL_ESSAYS-ML.xls\n"
     ]
    }
   ],
   "source": [
    "adversarial_dir = \"Data Sets/adversarial_asap\"\n",
    "test_set_file = adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-ML.xls\"\n",
    "\n",
    "#verify data paths\n",
    "print(test_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGrAt_cTcGMa"
   },
   "outputs": [],
   "source": [
    "# Attack 1: Shuffling Words\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_shuffle = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_shuffle = test_set_shuffle.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_shuffle.index: \n",
    "  words= test_set_shuffle.at[i, 'essay'].split()\n",
    "  random.shuffle(words)\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_shuffle.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_shuffle.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SHUFFLE.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcCzhtcOnsQT"
   },
   "source": [
    "### Anchor: Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z86ygQTPX4vU"
   },
   "outputs": [],
   "source": [
    "# Attack 2a: Appending - \"Library\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_append = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_append = test_set_append.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_append.index: \n",
    "  words= test_set_append.at[i, 'essay'].split()\n",
    "  words.append(\"library\")\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_append.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_append.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-APPEND_LIBRARY.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LsXUQ6NcPwG"
   },
   "outputs": [],
   "source": [
    "# Attack 3a: Progressive Overload - \"Library\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_progressive = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_progressive = test_set_progressive.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_progressive.index: \n",
    "  words= test_set_progressive.at[i, 'essay'].split()\n",
    "  if i < 591:\n",
    "    continue\n",
    "  if i < 641:\n",
    "    for x in range(0,i-590):\n",
    "      words[x] = \"library\"\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_progressive.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_progressive.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-PROGRESSIVE_LIBRARY.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4t3-QNZPkenL"
   },
   "outputs": [],
   "source": [
    "# Attack 4a: Single Substitution - \"Library\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_single = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_single = test_set_single.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_single.index: \n",
    "  words= test_set_single.at[i, 'essay'].split()\n",
    "  if i < 591:\n",
    "    continue\n",
    "  if i < 641:\n",
    "    words[i-591] = \"library\"\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_single.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_single.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SINGLE_LIBRARY.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVYtrlvRm_lq"
   },
   "outputs": [],
   "source": [
    "# Attack 5a: Insertion of anchor in random locations\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_insertion = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_insertion = test_set_insertion.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_insertion.index: \n",
    "  words= test_set_insertion.at[i, 'essay'].split()\n",
    "  x = random.randint(0,len(words))\n",
    "  words.insert(x, 'library')\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_insertion.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_insertion.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-INSERTION_LIBRARY.xls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vgcvRskoOvR"
   },
   "source": [
    "### Anchor: Censorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-NTaSu1QukN"
   },
   "outputs": [],
   "source": [
    "# Attack 2b: Appending - \"Censorship\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_append = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_append = test_set_append.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_append.index: \n",
    "  words= test_set_append.at[i, 'essay'].split()\n",
    "  words.append(\"censorship\")\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_append.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_append.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-APPEND_CENSORSHIP.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lyVZ61w4P9Tu"
   },
   "outputs": [],
   "source": [
    "# Attack 3b: Progressive Overload - \"Censorship\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_progressive = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_progressive = test_set_progressive.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_progressive.index: \n",
    "  words= test_set_progressive.at[i, 'essay'].split()\n",
    "  if i < 591:\n",
    "    continue\n",
    "  if i < 641:\n",
    "    for x in range(0,i-590):\n",
    "      words[x] = \"censorship\"\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_progressive.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_progressive.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-PROGRESSIVE_CENSORSHIP.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gV_naQSbRJ4K"
   },
   "outputs": [],
   "source": [
    "# Attack 4b: Single Substitution - \"Censorship\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_single = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_single = test_set_single.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_single.index: \n",
    "  words= test_set_single.at[i, 'essay'].split()\n",
    "  if i < 591:\n",
    "    continue\n",
    "  if i < 641:\n",
    "    words[i-591] = \"censorship\"\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_single.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_single.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SINGLE_CENSORSHIP.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2E0Qci80R1MQ"
   },
   "outputs": [],
   "source": [
    "# Attack 5b: Insertion of \"censorship\" in random locations\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_insertion = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_insertion = test_set_insertion.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_insertion.index: \n",
    "  words= test_set_insertion.at[i, 'essay'].split()\n",
    "  x = random.randint(0,len(words))\n",
    "  words.insert(x, 'censorship')\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_insertion.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_insertion.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-INSERTION_CENSORSHIP.xls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nD0AW2kboS9z"
   },
   "source": [
    "### Anchor: The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aunh5fOioaAw"
   },
   "outputs": [],
   "source": [
    "# Attack 2c: Appending - \"The\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_append = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_append = test_set_append.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_append.index: \n",
    "  words= test_set_append.at[i, 'essay'].split()\n",
    "  words.append(\"the\")\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_append.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_append.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-APPEND_THE.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZD4Z1nSowBo"
   },
   "outputs": [],
   "source": [
    "# Attack 3c: Progressive Overload - \"The\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_progressive = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_progressive = test_set_progressive.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_progressive.index: \n",
    "  words= test_set_progressive.at[i, 'essay'].split()\n",
    "  if i < 591:\n",
    "    continue\n",
    "  if i < 641:\n",
    "    for x in range(0,i-590):\n",
    "      words[x] = \"the\"\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_progressive.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_progressive.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-PROGRESSIVE_THE.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nPPc1FOoyck"
   },
   "outputs": [],
   "source": [
    "# Attack 4c: Single Substitution - \"The\"\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_single = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_single = test_set_single.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_single.index: \n",
    "  words= test_set_single.at[i, 'essay'].split()\n",
    "  if i < 591:\n",
    "    continue\n",
    "  if i < 641:\n",
    "    words[i-591] = \"censorship\"\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_single.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_single.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SINGLE_THE.xls\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huUANQo-o3dX"
   },
   "outputs": [],
   "source": [
    "# Attack 5c: Insertion of \"the\" in random locations\n",
    "\n",
    "#load excel into dataframe\n",
    "test_set_insertion = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "\n",
    "#remove empty n/a cells\n",
    "test_set_insertion = test_set_insertion.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "\n",
    "for i in test_set_insertion.index: \n",
    "  words= test_set_insertion.at[i, 'essay'].split()\n",
    "  x = random.randint(0,len(words))\n",
    "  words.insert(x, 'the')\n",
    "  new_sentence = ' '.join(words)\n",
    "  test_set_insertion.at[i,'essay'] = new_sentence  \n",
    "  \n",
    "test_set_insertion.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-INSERTION_THE.xls\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Adversarial Attack Perturbation Creator.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
