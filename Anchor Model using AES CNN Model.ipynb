{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRrmgI3cpw2D"
   },
   "source": [
    "\n",
    "# HU Extension    ---      Final Project    ---   S89A DL for NLP\n",
    "# Michael Lee & Micah Nickerson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukFvFIcnpwsb"
   },
   "source": [
    "# PART 2A - ANCHOR MODEL GENERATION\n",
    "\n",
    "This notebook **finds Anchor Words** that predict high scores, to optimize Adversarial Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WP2PgbzEp5bn"
   },
   "source": [
    "# Project Master Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LeGnxyaqqZqv"
   },
   "outputs": [],
   "source": [
    "#Data Storage Parameters\n",
    "dataset_dir = \"Data Sets/asap-aes\"\n",
    "adversarial_dir = \"Data Sets/adversarial_asap\"\n",
    "model_save_dir = \"Model Files\"\n",
    "selected_essay_id = 2\n",
    "training_set_file = dataset_dir+\"/training_set_rel3.xls\" \n",
    "\n",
    "###Test sets:\n",
    "test_set_file = dataset_dir+\"/valid_set.xls\"\n",
    "test_set_scores_file = dataset_dir+\"/valid_sample_submission_5_column.csv\"\n",
    "\n",
    "#Data Embedding Parameters\n",
    "# Take First X words from each essay, abandon rest\n",
    "max_len = 1118 #longest essay \n",
    "\n",
    "# Word Dimensionality - consider the top 15,000 words in the dataset\n",
    "max_words = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9v0JEGwy7X8"
   },
   "outputs": [],
   "source": [
    "def make_prediction(modelname,sampess):\n",
    "    sample_prediction = modelname.predict(test_set_essays_emb[sampess:sampess+1])\n",
    "    return sample_prediction\n",
    "  \n",
    "def calculate_score(prediction):\n",
    "    score = {}\n",
    "    score[1]=prediction[0,0]\n",
    "    score[2]=prediction[0,1]\n",
    "    score[3]=prediction[0,2]    \n",
    "    score[4]=prediction[0,3]  \n",
    "    score[5]=prediction[0,4]    \n",
    "    score[6]=prediction[0,5]    \n",
    "    calculate_score = max(score, key=score.get)\n",
    "    return(calculate_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VziCcGc-rAtO"
   },
   "source": [
    "### Load Packages and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4454,
     "status": "ok",
     "timestamp": 1564708783008,
     "user": {
      "displayName": "Michael Lee",
      "photoUrl": "",
      "userId": "10845871506120063440"
     },
     "user_tz": 240
    },
    "id": "3-V6A6nPrAQd",
    "outputId": "12480a09-6abc-476c-d6ff-1cb1e97a2163"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#data loading\n",
    "import os\n",
    "\n",
    "# python modules\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "\n",
    "####data manipulation####\n",
    "import numpy as np\n",
    "from numpy.random import shuffle \n",
    "import pandas as pd\n",
    "\n",
    "####word2vec encoding####\n",
    "import gensim\n",
    "\n",
    "####data visualization####\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "####CNN tools####\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1abxZSVqBB3"
   },
   "source": [
    "### Load and Clean Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37857,
     "status": "ok",
     "timestamp": 1564961276968,
     "user": {
      "displayName": "Micah Nickerson",
      "photoUrl": "https://lh6.googleusercontent.com/-oyKtz01VaLs/AAAAAAAAAAI/AAAAAAAAAAc/VYhOjJJ64lg/s64/photo.jpg",
      "userId": "14296761342348940958"
     },
     "user_tz": 240
    },
    "id": "MKR_zcROqEJb",
    "outputId": "93f70ae5-e83f-43df-ca7e-57038f6443bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AL_XhzLy0RPk"
   },
   "source": [
    "- Load, Filter and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 281161,
     "status": "ok",
     "timestamp": 1564709059776,
     "user": {
      "displayName": "Michael Lee",
      "photoUrl": "",
      "userId": "10845871506120063440"
     },
     "user_tz": 240
    },
    "id": "ByfHvyqPpp-D",
    "outputId": "cd943b60-4638-4add-bfba-3a9709bbc62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/Shared drives/CSCI S-89A - Group Project/Data Sets/asap-aes/training_set_rel3.xls\n",
      "/content/drive/Shared drives/CSCI S-89A - Group Project/Data Sets/adversarial_asap/valid_set_plus_ADVERSARIAL_ESSAYS-ML.xls\n",
      "\n",
      "Entire Corpus for ASAP:\n",
      "Training Set: (12978, 28)\n",
      "Test Set: (4219, 5) \n",
      "\n",
      "Selected Essay Set #2 Corpus:\n",
      "Training Set: (1800, 9)\n",
      "Test Set: (601, 4)\n",
      "Total Data Set: 2401\n"
     ]
    }
   ],
   "source": [
    "#verify data paths\n",
    "print(training_set_file)\n",
    "print(test_set_file)\n",
    "\n",
    "#load excel into dataframe\n",
    "raw_training_set = pd.read_excel(training_set_file, sheet_name='training_set')\n",
    "test_set = pd.read_excel(test_set_file, sheet_name='valid_set')\n",
    "test_set_scores = pd.read_csv(test_set_scores_file)\n",
    "\n",
    "print(\"\\nEntire Corpus for ASAP:\")\n",
    "print(\"Training Set:\",raw_training_set.shape)\n",
    "#print(\"Validation:\",valid_set.shape)\n",
    "print(\"Test Set:\",test_set.shape,\"\\n\")\n",
    "\n",
    "#filter data by essay set\n",
    "essay_fltr =  raw_training_set['essay_set']== selected_essay_id\n",
    "training_set = raw_training_set[essay_fltr]\n",
    "\n",
    "essay_fltr =  test_set['essay_set']== selected_essay_id\n",
    "test_set = test_set[essay_fltr]\n",
    "\n",
    "essay_fltr =  test_set_scores['essay_set']== selected_essay_id\n",
    "test_set_scores = test_set_scores[essay_fltr]\n",
    "\n",
    "#remove empty n/a cells\n",
    "training_set = training_set.drop(['rater3_domain1','rater1_trait1','rater1_trait2','rater1_trait3','rater1_trait4','rater1_trait5','rater1_trait6','rater2_trait1','rater2_trait2','rater2_trait3','rater2_trait4','rater2_trait5','rater2_trait6','rater3_trait1','rater3_trait2','rater3_trait3','rater3_trait4','rater3_trait5','rater3_trait6'], axis=1)    \n",
    "test_set = test_set.drop(['domain2_predictionid'], axis=1)    \n",
    "\n",
    "training_set_top = training_set.head()\n",
    "#print(training_set_top)\n",
    "test_set_top = test_set.head()\n",
    "#print(test_set_top)\n",
    "\n",
    "#3 sets, training, validation and testing\n",
    "\n",
    "print(\"Selected Essay Set #%s Corpus:\" % selected_essay_id)\n",
    "print(\"Training Set:\",training_set.shape)\n",
    "print(\"Test Set:\",test_set.shape)\n",
    "print(\"Total Data Set:\", training_set.shape[0]+test_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxzNn_2yrxEK"
   },
   "source": [
    "- Split data into Essay and Label Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQhUBo7Zrwb6"
   },
   "outputs": [],
   "source": [
    "#extract essays and convert to NumPy for Keras\n",
    "training_set_essays = training_set['essay']\n",
    "training_set_essays = training_set_essays.values\n",
    "test_set_essays = test_set['essay']\n",
    "test_set_essays = test_set_essays.values\n",
    "\n",
    "#extract scores and convert to NumPy for Keras\n",
    "training_set_dom1scores = training_set['domain1_score']\n",
    "training_set_dom1scores = training_set_dom1scores.values\n",
    "\n",
    "#extract domain#1 predicted scores\n",
    "#data cleaning due to strange score input shape\n",
    "test_set_dom1scores = []\n",
    "for i in (range(test_set_scores.shape[0])):\n",
    "    if (i % 2) == 0: #print every other cell, since second cell is domain#2\n",
    "        asdf = test_set_scores['predicted_score'].values[i]\n",
    "        i_score_no = float(asdf)\n",
    "        #print(asdf)\n",
    "        #test_set_dom1scores = test_set_dom1scores.append({'predicted_score': asdf}, ignore_index=True)\n",
    "        test_set_dom1scores.append(i_score_no)\n",
    "#convert to NumPy Array\n",
    "test_set_dom1scores = np.asarray(test_set_dom1scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BdjL-MkrD1N"
   },
   "source": [
    "### Encoding Essays\n",
    "\n",
    "- Tokenization and Word Indexing of Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 283956,
     "status": "ok",
     "timestamp": 1564709062747,
     "user": {
      "displayName": "Michael Lee",
      "photoUrl": "",
      "userId": "10845871506120063440"
     },
     "user_tz": 240
    },
    "id": "MryRzF45vJWx",
    "outputId": "e8f80072-0194-4731-af91-6b5bc6225f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17023 unique tokens.\n",
      "Shape of Training data tensor: (1800, 1118)\n",
      "Shape of Testing data tensor: (601, 1118)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the Essays\n",
    "\n",
    "#TEMPORARILY COMBINE TRAIN AND TEST TO SIMPLIFY EMBEDDING PROCESS\n",
    "#single embedding process, max token index\n",
    "lengthmark = len(training_set_essays)\n",
    "combined_essays = np.append(training_set_essays,test_set_essays)\n",
    "\n",
    "# Tokenize the data \n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(combined_essays)\n",
    "sequences = tokenizer.texts_to_sequences(combined_essays)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Pad sequences that are shorter than others\n",
    "combined_data_pen = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "#SPLIT TRAINING AND TEST SETS BACK\n",
    "train_data_pen = combined_data_pen[:lengthmark]\n",
    "test_data_pen = combined_data_pen[lengthmark:]\n",
    "\n",
    "# Load the label\n",
    "print('Shape of Testing data tensor:', test_data_pen.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRFGq89L902P"
   },
   "source": [
    "- One Hot Encoding of Essay Scores 1-6\n",
    "\n",
    "    * **2** = 010000\n",
    "\n",
    "    * **6** = 000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 283939,
     "status": "ok",
     "timestamp": 1564709062748,
     "user": {
      "displayName": "Michael Lee",
      "photoUrl": "",
      "userId": "10845871506120063440"
     },
     "user_tz": 240
    },
    "id": "0oLx5DzJA9Gv",
    "outputId": "82d7b43f-5735-49d1-85ca-bff2096c49c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels Shape: (1800, 6)\n",
      "Test Labels Shape: (601, 6)\n"
     ]
    }
   ],
   "source": [
    "train_labels_pen = np.zeros((0, 6))\n",
    "\n",
    "#Scores to Dummy Variable Conversion\n",
    "#Training (and Validation) Set\n",
    "for item in training_set_dom1scores:\n",
    "      if item==1:\n",
    "          train_labels_pen = np.append(train_labels_pen, [[1,0,0,0,0,0]],axis = 0)\n",
    "      elif item==2:\n",
    "          train_labels_pen = np.append(train_labels_pen, [[0,1,0,0,0,0]],axis = 0)        \n",
    "      elif item==3:\n",
    "          train_labels_pen = np.append(train_labels_pen, [[0,0,1,0,0,0]],axis = 0)        \n",
    "      elif item==4:\n",
    "          train_labels_pen = np.append(train_labels_pen, [[0,0,0,1,0,0]],axis = 0)        \n",
    "      elif item==5:\n",
    "          train_labels_pen = np.append(train_labels_pen, [[0,0,0,0,1,0]],axis = 0)        \n",
    "      else:\n",
    "          train_labels_pen = np.append(train_labels_pen, [[0,0,0,0,0,1]],axis = 0)   \n",
    "\n",
    "test_labels_pen = np.zeros((0, 6))\n",
    "\n",
    "#Scores to Dummy Variable Conversion\n",
    "#Testing Set\n",
    "for item in test_set_dom1scores:\n",
    "      if item==1:\n",
    "          test_labels_pen = np.append(test_labels_pen, [[1,0,0,0,0,0]],axis = 0)\n",
    "      elif item==2:\n",
    "          test_labels_pen = np.append(test_labels_pen, [[0,1,0,0,0,0]],axis = 0)        \n",
    "      elif item==3:\n",
    "          test_labels_pen = np.append(test_labels_pen, [[0,0,1,0,0,0]],axis = 0)        \n",
    "      elif item==4:\n",
    "          test_labels_pen = np.append(test_labels_pen, [[0,0,0,1,0,0]],axis = 0)        \n",
    "      elif item==5:\n",
    "          test_labels_pen = np.append(test_labels_pen, [[0,0,0,0,1,0]],axis = 0)        \n",
    "      else:\n",
    "          test_labels_pen = np.append(test_labels_pen, [[0,0,0,0,0,1]],axis = 0)   \n",
    "          \n",
    "print(\"Test Labels Shape:\" ,test_labels_pen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 283919,
     "status": "ok",
     "timestamp": 1564709062749,
     "user": {
      "displayName": "Michael Lee",
      "photoUrl": "",
      "userId": "10845871506120063440"
     },
     "user_tz": 240
    },
    "id": "uytH8z-PvJKR",
    "outputId": "5b8c6c94-949b-4864-f5ad-0978d2b723c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Essays and matching Scores:\n",
      "Shape:  (1440, 1118) (1440, 6)\n",
      "\n",
      "Validation Set Essays and matching Scores:\n",
      "Shape:  (360, 1118) (360, 6)\n",
      "\n",
      "Test Set Essays and matching Scores:\n",
      "Shape:  (601, 1118) (601, 6)\n"
     ]
    }
   ],
   "source": [
    "#TEST SET IS LEFT ALONE\n",
    "\n",
    "val_set_essays = training_set_essays \n",
    "val_set_dom1scores = training_set_dom1scores\n",
    "\n",
    "#split coded scores\n",
    "set_split_test = int((len(train_data_pen))*test_split)\n",
    "training_set_essays_emb, val_set_essays_emb = train_data_pen[:set_split_test], train_data_pen[set_split_test:]\n",
    "training_set_dom1scores_emb, val_set_dom1scores_emb = train_labels_pen[:set_split_test], train_labels_pen[set_split_test:]\n",
    "#split the unencoded scores\n",
    "training_set_dom1scores, val_set_dom1scores = training_set_dom1scores[:set_split_test], training_set_dom1scores[set_split_test:]\n",
    "\n",
    "test_set_essays_emb = test_data_pen\n",
    "test_set_dom1scores_emb = test_labels_pen\n",
    "\n",
    "print(\"\\nTest Set Essays and matching Scores:\")\n",
    "print(\"Shape: \",test_set_essays_emb.shape, test_set_dom1scores_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--1HZAkg1Kh0"
   },
   "source": [
    "- Embedding Essays using GloVe Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OjAco_z9QnP"
   },
   "source": [
    "# Load Black Box in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0GeSNwJLnFV"
   },
   "outputs": [],
   "source": [
    "test_model_black_box = load_model(model_save_dir+'/D1_76_BLACKBOX_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nPExFFtrL1x"
   },
   "source": [
    "# Anchor Identification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vadr_LodemJk"
   },
   "source": [
    "- Finding Anchor Words that predict high scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Oyfa-VMekTD"
   },
   "outputs": [],
   "source": [
    "!pip install anchor_exp\n",
    "!pip install -q spacy && python -m spacy download en_core_web_lg && python -m spacy link en_core_web_lg enlg\n",
    "\n",
    "from anchor import anchor_text\n",
    "\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('enlg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IlCl6z6esQU"
   },
   "outputs": [],
   "source": [
    "sample_ids = [0]\n",
    "\n",
    "for idx in sample_ids:\n",
    "    print('Index: %d, Feature: %s' % (idx, test_set_essays[idx]))\n",
    "    print('True Score: %s' % (test_set_dom1scores[idx]))\n",
    "    estimatedscore=[calculate_score(make_prediction(test_model_black_box, idx))]\n",
    "    def estimator(estimatedscore):\n",
    "      estimator = np.asarray(estimatedscore)\n",
    "      return estimator\n",
    "\n",
    "    #classifier_fn([text])[0]\n",
    "\n",
    "    explainer = anchor_text.AnchorText(spacy_nlp, [1,2,3,4,5,6], use_unk_distribution=True)\n",
    "    exp = explainer.explain_instance(test_set_essays[idx], estimator, threshold=0.95, use_proba=True, batch_size=30)\n",
    "\n",
    "    max_pred = 2\n",
    "    print('Key Signal from Anchors: %s' % (' AND '.join(exp.names())))\n",
    "    print(exp.features())\n",
    "    print('Precision: %.2f' % exp.precision())\n",
    "    print()\n",
    "\n",
    "    #exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59pg6UhdHpH1"
   },
   "outputs": [],
   "source": [
    "#using a real essay\n",
    "\n",
    "sample_ids = [0]\n",
    "\n",
    "for idx in sample_ids:\n",
    "    print('Index: %d, Feature: %s' % (idx, training_set_essays[idx]))\n",
    "    print('True Score: %s' % (training_set_dom1scores[idx]))\n",
    "    estimatedscore=[calculate_score(make_prediction(test_model_black_box, idx))]\n",
    "    def estimator(estimatedscore):\n",
    "      estimator = np.asarray(estimatedscore)\n",
    "      return estimator\n",
    "\n",
    "    #classifier_fn([text])[0]\n",
    "\n",
    "    explainer = anchor_text.AnchorText(spacy_nlp, [1,2,3,4,5,6], use_unk_distribution=True)\n",
    "    exp = explainer.explain_instance(training_set_essays[idx], estimator, threshold=0.8, use_proba=True, batch_size=30)\n",
    "\n",
    "    max_pred = 2\n",
    "    print('Key Signal from Anchors: %s' % (' AND '.join(exp.names())))\n",
    "    print('Precision: %.2f' % exp.precision())\n",
    "    print()\n",
    "\n",
    "    #exp.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WP2PgbzEp5bn",
    "VziCcGc-rAtO",
    "D1abxZSVqBB3",
    "_BdjL-MkrD1N"
   ],
   "machine_shape": "hm",
   "name": "Anchor Model using AES CNN Model.ipynb",
   "provenance": [
    {
     "file_id": "1T2GWzzaS13G76dhmDT1JWpn9JNwnebNt",
     "timestamp": 1564634333221
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
